{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = 'a89228ac-8bf4-4646-9d2c-442b1cb5d622'\n",
    "resource_group = 'lauri-ml'\n",
    "aml_workspace = 'lauri-ml'\n",
    "cluster_name = 'bert-gpu'\n",
    "# azure_dataset_name = 'Azure Services Dataset'\n",
    "# azure_dataset_path = 'azure-service-classifier/data'\n",
    "# azure_dataset_descr = 'Dataset containing azure related posts on Stackoverflow'\n",
    "experiment_name = 'bert-classifier-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace #, Dataset, Experiment\n",
    "# from azureml.core.runconfig import RunConfiguration\n",
    "# from azureml.widgets import RunDetails\n",
    "# from azureml.train.dnn import TensorFlow\n",
    "from model import TFBertForMultiClassification\n",
    "from transformers import BertTokenizer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Azure Machine Learning Python SDK version\n",
    "\n",
    "This tutorial requires version 1.0.69 or higher. Let's check the version of the SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Azure Machine Learning Python SDK version: 1.0.85\n"
    }
   ],
   "source": [
    "import azureml.core\n",
    "\n",
    "print(\"Azure Machine Learning Python SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect To Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aml_workspace = Workspace(\n",
    "#     subscription_id=subscription_id, resource_group=resource_group, workspace_name=aml_workspace\n",
    "# )\n",
    "# aml_workspace.write_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Performing interactive authentication. Please follow the instructions on the terminal.\nWARNING - Note, we have launched a browser for you to login. For old experience with device code, use \"az login --use-device-code\"\nWARNING - You have logged in. Now let us find all the subscriptions to which you have access...\nInteractive authentication successfully completed.\nWorkspace name: lauri-ml\nAzure region: westeurope\nSubscription id: a89228ac-8bf4-4646-9d2c-442b1cb5d622\nResource group: lauri-ml\n"
    }
   ],
   "source": [
    "# from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "# interactive_auth = InteractiveLoginAuthentication()\n",
    "# workspace = Workspace.from_config(auth=interactive_auth)\n",
    "\n",
    "workspace = Workspace.from_config()\n",
    "print('Workspace name: ' + workspace.name, \n",
    "      'Azure region: ' + workspace.location, \n",
    "      'Subscription id: ' + workspace.subscription_id, \n",
    "      'Resource group: ' + workspace.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Compute Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the compute target has already been created, then you (and other users in your workspace) can directly run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_target = workspace.compute_targets[cluster_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can also use a local compute target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_runcfg = RunConfiguration()\n",
    "local_runcfg.environment.python.user_managed_dependencies = True\n",
    "compute_target = local_runcfg.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# datastore_name = 'tfworld'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# from azureml.core import Datastore, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the datastore has already been registered, then you (and other users in your workspace) can directly run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datastore = workspace.datastores[datastore_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the dataset has already been registered, then you (and other users in your workspace) can directly run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# azure_dataset = workspace.datasets[azure_dataset_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Experiment\n",
    "\n",
    "Now that we have our compute target, dataset, and training script working locally, it is time to scale up so that the script can run faster. We will start by creating an [experiment](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment.experiment?view=azure-ml-py). An experiment is a grouping of many runs from a specified script. All runs in this tutorial will be performed under the same experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(workspace, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.download_files(prefix='../outputs/model')\n",
    "\n",
    "# If you haven't finished training the model then just download pre-made model from datastore\n",
    "# datastore.download('./',prefix=\"azure-service-classifier/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the model\n",
    "\n",
    "Next step is to import our model class and instantiate fine-tuned model from the model file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_example(text, max_seq_length):\n",
    "    # Encode inputs using tokenizer\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_seq_length\n",
    "    )\n",
    "    input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_seq_length - len(input_ids)\n",
    "    input_ids = input_ids + ([0] * padding_length)\n",
    "    attention_mask = attention_mask + ([0] * padding_length)\n",
    "    token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "    \n",
    "    return input_ids, attention_mask, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from disk.\n"
     ]
    }
   ],
   "source": [
    "labels = ['azure-web-app-service', 'azure-storage', 'azure-devops', 'azure-virtual-machine', 'azure-functions']\n",
    "# Load model and tokenizer\n",
    "# loaded_model = TFBertForMultiClassification.from_pretrained('azure-service-classifier/model', num_labels=len(labels))\n",
    "loaded_model = TFBertForMultiClassification.from_pretrained('../outputs/model', num_labels=len(labels))\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "print(\"Model loaded from disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define prediction function\n",
    "\n",
    "Using the model object we can interpret new questions and predict what Azure service they talk about. To do that conveniently we'll define **predict** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict(question):\n",
    "    input_ids, attention_mask, token_type_ids = encode_example(question, 128)\n",
    "    predictions = loaded_model.predict({\n",
    "        'input_ids': tf.convert_to_tensor([input_ids], dtype=tf.int32),\n",
    "        'attention_mask': tf.convert_to_tensor([attention_mask], dtype=tf.int32),\n",
    "        'token_type_ids': tf.convert_to_tensor([token_type_ids], dtype=tf.int32)\n",
    "    })\n",
    "    prediction = labels[predictions[0].argmax().item()]\n",
    "    probability = predictions[0].max()\n",
    "    result = {\n",
    "        'prediction': str(labels[predictions[0].argmax().item()]),\n",
    "        'probability': str(predictions[0].max())\n",
    "    }\n",
    "    print('Prediction: {}'.format(prediction))\n",
    "    print('Probability: {}'.format(probability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiement with our new model\n",
    "\n",
    "Now we can easily test responses of the model to new inputs. \n",
    "*  **ACTION**: Invent yout own input for one of the 5 services our model understands: 'azure-web-app-service', 'azure-storage', 'azure-devops', 'azure-virtual-machine', 'azure-functions'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: azure-devops\n",
      "Probability: 0.2559393048286438\n"
     ]
    }
   ],
   "source": [
    "# Route question\n",
    "predict(\"How can I specify Service Principal in devops pipeline when deploying virtual machine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: azure-devops\n",
      "Probability: 0.2656690180301666\n"
     ]
    }
   ],
   "source": [
    "# Now more tricky cae - the opposite\n",
    "predict(\"How can virtual machine trigger devops pipeline\")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}