{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT fine-tuning with Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subscription_id = 'a89228ac-8bf4-4646-9d2c-442b1cb5d622'\n",
    "# resource_group = 'lauri-ml'\n",
    "# aml_workspace = 'lauri-ml'\n",
    "# cluster_name = 'bert-gpu'\n",
    "# azure_dataset_name = 'Azure Services Dataset'\n",
    "# azure_dataset_path = 'azure-service-classifier/data'\n",
    "# azure_dataset_descr = 'Dataset containing azure related posts on Stackoverflow'\n",
    "# data_dir = 'C:/Users/LauriLehman/Documents/Projects/bert-ms-lauri/1-Training/data'\n",
    "dataset_remotepath = 'bert-testing/training/'\n",
    "dataset_name = 'BERT training'\n",
    "experiment_name = 'bert-classifier-test'\n",
    "training_script = 'scripts/train.py'\n",
    "model_output_dir = '../outputs/model'\n",
    "# local_python_path = 'C:\\\\Users\\\\LauriLehman\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python36\\\\python.exe'\n",
    "local_python_path = 'C:/Users/LauriLehman/AppData/Local/Microsoft/WindowsApps/python.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset, Experiment, Environment\n",
    "from azureml.core.environment import CondaDependencies # PythonSection, DockerSection\n",
    "from azureml.core.runconfig import RunConfiguration, DEFAULT_GPU_IMAGE\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.dnn import TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment status check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Python runtime version: 3.7.6\n"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print('Python runtime version: {}.{}.{}'.format(sys.version_info[0], sys.version_info[1], sys.version_info[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Azure Machine Learning Python SDK version\n",
    "\n",
    "This tutorial requires version 1.0.69 or higher. Let's check the version of the SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Azure Machine Learning Python SDK version: 1.0.85\n"
    }
   ],
   "source": [
    "import azureml.core\n",
    "\n",
    "print('Azure Machine Learning Python SDK version:', azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect To Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aml_workspace = Workspace(\n",
    "#     subscription_id=subscription_id, resource_group=resource_group, workspace_name=aml_workspace\n",
    "# )\n",
    "# aml_workspace.write_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Workspace name: lauri-ml\nAzure region: westeurope\nSubscription id: a89228ac-8bf4-4646-9d2c-442b1cb5d622\nResource group: lauri-ml\n"
    }
   ],
   "source": [
    "# from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "# interactive_auth = InteractiveLoginAuthentication()\n",
    "# workspace = Workspace.from_config(auth=interactive_auth)\n",
    "\n",
    "workspace = Workspace.from_config()\n",
    "print('Workspace name: ' + workspace.name, \n",
    "      'Azure region: ' + workspace.location, \n",
    "      'Subscription id: ' + workspace.subscription_id, \n",
    "      'Resource group: ' + workspace.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Compute Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the compute target has already been created, then you (and other users in your workspace) can directly run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_target = workspace.compute_targets[cluster_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use a local compute target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_runcfg = RunConfiguration()\n",
    "compute_target = local_runcfg.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python_cfg = PythonSection()\n",
    "# python_cfg.interpreter_path = local_python_path\n",
    "# python_cfg.user_managed_dependencies = True\n",
    "# aml_env = Environment('local-lauri')\n",
    "# aml_env.python = python_cfg\n",
    "\n",
    "# local_runcfg.environment.python.interpreter_path = local_python_path\n",
    "# local_runcfg.environment.python.user_managed_dependencies = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker_cfg = DockerSection()\n",
    "# docker_cfg.enabled = True\n",
    "# # docker_cfg.base_image = 'base-gpu:openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04'\n",
    "# # docker_cfg.base_image = 'mcr.microsoft.com\\azureml\\base-gpu:openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04'\n",
    "# docker_cfg.base_image = 'mcr.microsoft.com/azureml/base-gpu:openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04'\n",
    "# aml_env = Environment('local-lauri')\n",
    "# aml_env.docker = docker_cfg\n",
    "\n",
    "local_runcfg.environment = Environment(name='tfenv')\n",
    "\n",
    "local_runcfg.environment.docker.enabled = True\n",
    "docker_image = 'mcr.microsoft.com/azureml/base-gpu:openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04'\n",
    "# docker_image = DEFAULT_GPU_IMAGE\n",
    "local_runcfg.environment.docker.base_image = docker_image\n",
    "# local_runcfg.environment.python.user_managed_dependencies = True\n",
    "# local_runcfg.environment.python.interpreter_path = local_python_path\n",
    "# ====\n",
    "# conda_dep = CondaDependencies()\n",
    "# conda_dep.add_conda_package('azureml-dataprep[fuse,pandas]')\n",
    "# conda_dep.add_conda_package('transformers')\n",
    "# conda_dep.add_tensorflow_conda_package(core_type='gpu', version=None)\n",
    "# ====\n",
    "# local_runcfg.environment.python.conda_dependencies.add_conda_package('azureml-dataprep[fuse,pandas]')\n",
    "# local_runcfg.environment.python.conda_dependencies.add_conda_package('transformers')\n",
    "# local_runcfg.environment.python.conda_dependencies.add_tensorflow_conda_package(core_type='gpu', version='2.0')\n",
    "\n",
    "local_runcfg.environment.python.conda_dependencies.add_pip_package('azureml-dataprep[fuse,pandas]')\n",
    "local_runcfg.environment.python.conda_dependencies.add_pip_package('transformers')\n",
    "local_runcfg.environment.python.conda_dependencies.add_tensorflow_pip_package(core_type='gpu', version='2.0')\n",
    "\n",
    "# local_runcfg.environment.python.conda_dependencies = CondaDependencies('C:/Users/LauriLehman/OneDrive - Kompozure Oy/Projektit/AzureMLDeploy/conda-tfenv-deps.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_env = local_runcfg.environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the datastore has already been registered, then you (and other users in your workspace) can directly run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datastore = workspace.datastores[datastore_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dataset has already been registered, then you (and other users in your workspace) can directly run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# azure_dataset = workspace.datasets[azure_dataset_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging in TensorFlow 2.0 Eager Mode\n",
    "\n",
    "Eager mode is new feature in TensorFlow 2.0 which makes understanding and debugging models easy. Let's start by configuring our remote debugging environment.\n",
    "\n",
    "#### Configure VS Code Remote connection to Notebook VM\n",
    "\n",
    "* **ACTION**: Install [Microsoft VS Code](https://code.visualstudio.com/) on your local machine.\n",
    "\n",
    "* **ACTION**: Follow this [configuration guide](https://github.com/danielsc/azureml-debug-training/blob/master/Setting%20up%20VSCode%20Remote%20on%20an%20AzureML%20Notebook%20VM.md) to setup VS Code Remote connection to Notebook VM.\n",
    "\n",
    "#### Debug training code using step-by-step debugger\n",
    "\n",
    "* **ACTION**: Open Remote VS Code session to your Notebook VM.\n",
    "* **ACTION**: Open file `/home/azureuser/cloudfiles/code/<username>/bert-stack-overflow/1-Training/train_eager.py`.\n",
    "* **ACTION**: Set break point in the file and start Python debugging session. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Experiment\n",
    "\n",
    "Now that we have our compute target, dataset, and training script working locally, it is time to scale up so that the script can run faster. We will start by creating an [experiment](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment.experiment?view=azure-ml-py). An experiment is a grouping of many runs from a specified script. All runs in this tutorial will be performed under the same experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(workspace, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create TensorFlow Estimator\n",
    "\n",
    "The Azure Machine Learning Python SDK Estimator classes allow you to easily construct run configurations for your experiments. They allow you too define parameters such as the training script to run, the compute target to run it on, framework versions, additional package requirements, etc. \n",
    "\n",
    "You can also use a generic [Estimator](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.estimator.estimator?view=azure-ml-py) to submit training scripts that use any learning framework you choose.\n",
    "\n",
    "For popular libaries like PyTorch and Tensorflow you can use their framework specific estimators. We will use the [TensorFlow Estimator](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.dnn.tensorflow?view=azure-ml-py) for our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_name = 'bert_training'\n",
    "\n",
    "datastore = workspace.get_default_datastore()\n",
    "dataref = DataReference(\n",
    "    datastore=datastore,\n",
    "    data_reference_name=input_name,\n",
    "    path_on_datastore=dataset_remotepath\n",
    ")\n",
    "# dataset = workspace.datasets[dataset_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "WARNING - If environment_definition or conda_dependencies_file_path is specified, Azure ML will not install any framework related packages on behalf of the user.\n"
    }
   ],
   "source": [
    "estimator = TensorFlow(\n",
    "    source_directory='.',\n",
    "    entry_script=training_script,\n",
    "    compute_target=compute_target, \n",
    "    script_params = {\n",
    "        # '--data_dir': dataset.as_named_input(input_name).as_mount(),\n",
    "        # '--data_dir': data_dir,\n",
    "        '--data_dir': dataref.as_download(),\n",
    "        '--max_seq_length': 128,\n",
    "        '--batch_size': 32,\n",
    "        '--learning_rate': 3e-5,\n",
    "        '--steps_per_epoch': 150,\n",
    "        '--num_epochs': 3,\n",
    "        '--export_dir': model_output_dir\n",
    "    },\n",
    "    framework_version='2.0',\n",
    "    environment_definition=aml_env\n",
    "    # use_gpu=True,\n",
    "    # use_docker=False,\n",
    "    # pip_packages=['transformers', 'azureml-dataprep[fuse,pandas]']\n",
    "    # pip_packages=['transformers==2.0.0', 'azureml-dataprep[fuse,pandas]==1.1.38']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "{\n    \"script\": \"scripts/train.py\",\n    \"arguments\": [\n        \"--data_dir\",\n        \"$AZUREML_DATAREFERENCE_bert_training\",\n        \"--max_seq_length\",\n        128,\n        \"--batch_size\",\n        32,\n        \"--learning_rate\",\n        3e-05,\n        \"--steps_per_epoch\",\n        150,\n        \"--num_epochs\",\n        3,\n        \"--export_dir\",\n        \"../outputs/model\"\n    ],\n    \"target\": \"local\",\n    \"framework\": \"Python\",\n    \"communicator\": \"None\",\n    \"maxRunDurationSeconds\": null,\n    \"nodeCount\": 1,\n    \"environment\": {\n        \"name\": \"tfenv\",\n        \"version\": null,\n        \"environmentVariables\": {\n            \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n        },\n        \"python\": {\n            \"userManagedDependencies\": false,\n            \"interpreterPath\": \"python\",\n            \"condaDependenciesFile\": null,\n            \"baseCondaEnvironment\": null,\n            \"condaDependencies\": {\n                \"name\": \"project_environment\",\n                \"dependencies\": [\n                    \"python=3.6.2\",\n                    {\n                        \"pip\": [\n                            \"azureml-defaults\",\n                            \"azureml-dataprep[fuse,pandas]\",\n                            \"transformers\",\n                            \"tensorflow-gpu==2.0\"\n                        ]\n                    }\n                ],\n                \"channels\": [\n                    \"conda-forge\"\n                ]\n            }\n        },\n        \"docker\": {\n            \"enabled\": true,\n            \"baseImage\": \"mcr.microsoft.com/azureml/base-gpu:openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04\",\n            \"baseDockerfile\": null,\n            \"sharedVolumes\": true,\n            \"shmSize\": \"2g\",\n            \"arguments\": [],\n            \"baseImageRegistry\": {\n                \"address\": null,\n                \"username\": null,\n                \"password\": null\n            }\n        },\n        \"spark\": {\n            \"repositories\": [],\n            \"packages\": [],\n            \"precachePackages\": true\n        },\n        \"databricks\": {\n            \"mavenLibraries\": [],\n            \"pypiLibraries\": [],\n            \"rcranLibraries\": [],\n            \"jarLibraries\": [],\n            \"eggLibraries\": []\n        },\n        \"inferencingStackVersion\": null\n    },\n    \"history\": {\n        \"outputCollection\": true,\n        \"snapshotProject\": true,\n        \"directoriesToWatch\": [\n            \"logs\"\n        ]\n    },\n    \"spark\": {\n        \"configuration\": {\n            \"spark.app.name\": \"Azure ML Experiment\",\n            \"spark.yarn.maxAppAttempts\": 1\n        }\n    },\n    \"hdi\": {\n        \"yarnDeployMode\": \"cluster\"\n    },\n    \"tensorflow\": {\n        \"workerCount\": 1,\n        \"parameterServerCount\": 1\n    },\n    \"mpi\": {\n        \"processCountPerNode\": 1\n    },\n    \"dataReferences\": {\n        \"bert_training\": {\n            \"dataStoreName\": \"workspaceblobstore\",\n            \"pathOnDataStore\": \"bert-testing/training/\",\n            \"mode\": \"download\",\n            \"overwrite\": false,\n            \"pathOnCompute\": null\n        }\n    },\n    \"data\": {},\n    \"sourceDirectoryDataStore\": null,\n    \"amlcompute\": {\n        \"vmSize\": null,\n        \"vmPriority\": null,\n        \"retainCluster\": false,\n        \"name\": null,\n        \"clusterMaxNodeCount\": 1\n    }\n}\n"
    }
   ],
   "source": [
    "print(estimator.run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick description for each of the parameters we have just defined:\n",
    "\n",
    "- `source_directory`: This specifies the root directory of our source code. \n",
    "- `entry_script`: This specifies the training script to run. It should be relative to the source_directory.\n",
    "- `compute_target`: This specifies to compute target to run the job on. We will use the one created earlier.\n",
    "- `script_params`: This specifies the input parameters to the training script. Please note:\n",
    "\n",
    "    1) *azure_dataset.as_named_input('azureservicedata').as_mount()* mounts the dataset to the remote compute and provides the path to the dataset on our datastore. \n",
    "    \n",
    "    2) All outputs from the training script must be outputted to an './outputs' directory as this is the only directory that will be saved to the run. \n",
    "    \n",
    "    \n",
    "- `framework_version`: This specifies the version of TensorFlow to use. Use Tensorflow.get_supported_verions() to see all supported versions.\n",
    "- `use_gpu`: This will use the GPU on the compute target for training if set to True.\n",
    "- `pip_packages`: This allows you to define any additional libraries to install before training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Add Metrics Logging\n",
    "\n",
    "So we were able to clone a Tensorflow 2.0 project and run it without any changes. However, with larger scale projects we would want to log some metrics in order to make it easier to monitor the performance of our model. \n",
    "\n",
    "We can do this by adding a few lines of code into our training script:\n",
    "\n",
    "```python\n",
    "# 1) Import SDK Run object\n",
    "from azureml.core.run import Run\n",
    "\n",
    "# 2) Get current service context\n",
    "run = Run.get_context()\n",
    "\n",
    "# 3) Log the metrics that we want\n",
    "run.log('val_accuracy', float(logs.get('val_accuracy')))\n",
    "run.log('accuracy', float(logs.get('accuracy')))\n",
    "```\n",
    "We've created a *train_logging.py* script that includes logging metrics as shown above. \n",
    "\n",
    "*  **ACTION**: Explore _train_logging.py_ using [Azure ML studio > Notebooks tab](images/azuremlstudio-notebooks-explore.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Submit First Run \n",
    "\n",
    "We can now train our model by submitting the estimator object as a [run](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run.run?view=azure-ml-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huom! Docker on oltava käynnissä ennen kuin ajo aloitetaan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the current status of the run and stream the logs from within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50270f1361184d02b99c1adcc2622cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"loading\": true}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You cancel a run at anytime which will stop the run and scale down the nodes in the compute target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we view the current details of the run, you will notice that the metrics will be logged into graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Monitoring metrics with Tensorboard\n",
    "\n",
    "Tensorboard is a popular Deep Learning Training visualization tool and it's built-in into TensorFlow framework. We can easily add tracking of the metrics in Tensorboard format by adding Tensorboard callback to the **fit** function call.\n",
    "```python\n",
    "    # Add callback to record Tensorboard events\n",
    "    model.fit(train_dataset, epochs=FLAGS.num_epochs, \n",
    "              steps_per_epoch=FLAGS.steps_per_epoch, validation_data=valid_dataset, \n",
    "              callbacks=[\n",
    "                  AmlLogger(),\n",
    "                  tf.keras.callbacks.TensorBoard(update_freq='batch')]\n",
    "             )\n",
    "```\n",
    "\n",
    "#### Launch Tensorboard\n",
    "Azure ML service provides built-in integration with Tensorboard through **tensorboard** package.\n",
    "\n",
    "While the run is in progress (or after it has completed), we can start Tensorboard with the run as its target, and it will begin streaming logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://lauri-notebooks-6006.westeurope.notebooks.azureml.net\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://lauri-notebooks-6006.westeurope.notebooks.azureml.net'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from azureml.tensorboard import Tensorboard\n",
    "\n",
    "# The Tensorboard constructor takes an array of runs, so be sure and pass it in as a single-element array here\n",
    "# tb = Tensorboard([run])\n",
    "\n",
    "# If successful, start() returns a string with the URI of the instance.\n",
    "# tb.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Tensorboard\n",
    "When you're done, make sure to call the stop() method of the Tensorboard object, or it will stay running even after your job completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tb.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the model performance\n",
    "\n",
    "Last training run produced model of decent accuracy. Let's test it out and see what it does. First, let's check what files our latest training run produced and download the model files.\n",
    "\n",
    "#### Download model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['azureml-logs/20_image_build_log.txt',\n",
       " 'azureml-logs/55_azureml-execution-tvmps_09fffe7baed25fd27e6e55140e10b7eafd54a9383e58feb74aa4d80ac7920972_d.txt',\n",
       " 'azureml-logs/65_job_prep-tvmps_09fffe7baed25fd27e6e55140e10b7eafd54a9383e58feb74aa4d80ac7920972_d.txt',\n",
       " 'azureml-logs/70_driver_log.txt',\n",
       " 'azureml-logs/75_job_post-tvmps_09fffe7baed25fd27e6e55140e10b7eafd54a9383e58feb74aa4d80ac7920972_d.txt',\n",
       " 'azureml-logs/process_info.json',\n",
       " 'azureml-logs/process_status.json',\n",
       " 'logs/azureml/129_azureml.log',\n",
       " 'logs/azureml/job_prep_azureml.log',\n",
       " 'logs/azureml/job_release_azureml.log',\n",
       " 'logs/train/events.out.tfevents.1581356306.66f9c6bf321f438ca2c8589644c412b6000000.129.11285.v2',\n",
       " 'logs/train/events.out.tfevents.1581356333.66f9c6bf321f438ca2c8589644c412b6000000.profile-empty',\n",
       " 'logs/train/plugins/profile/2020-02-10_17-38-53/local.trace',\n",
       " 'logs/validation/events.out.tfevents.1581356679.66f9c6bf321f438ca2c8589644c412b6000000.129.42775.v2',\n",
       " 'outputs/model/config.json',\n",
       " 'outputs/model/tf_model.h5']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.download_files(prefix='../outputs/model')\n",
    "\n",
    "# If you haven't finished training the model then just download pre-made model from datastore\n",
    "# datastore.download('./',prefix=\"azure-service-classifier/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the model\n",
    "\n",
    "Next step is to import our model class and instantiate fine-tuned model from the model file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import TFBertForMultiClassification\n",
    "from transformers import BertTokenizer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_example(text, max_seq_length):\n",
    "    # Encode inputs using tokenizer\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_seq_length\n",
    "        )\n",
    "    input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_seq_length - len(input_ids)\n",
    "    input_ids = input_ids + ([0] * padding_length)\n",
    "    attention_mask = attention_mask + ([0] * padding_length)\n",
    "    token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "    \n",
    "    return input_ids, attention_mask, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from disk.\n"
     ]
    }
   ],
   "source": [
    "labels = ['azure-web-app-service', 'azure-storage', 'azure-devops', 'azure-virtual-machine', 'azure-functions']\n",
    "# Load model and tokenizer\n",
    "# loaded_model = TFBertForMultiClassification.from_pretrained('azure-service-classifier/model', num_labels=len(labels))\n",
    "loaded_model = TFBertForMultiClassification.from_pretrained('../outputs/model', num_labels=len(labels))\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "print(\"Model loaded from disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define prediction function\n",
    "\n",
    "Using the model object we can interpret new questions and predict what Azure service they talk about. To do that conveniently we'll define **predict** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict(question):\n",
    "    input_ids, attention_mask, token_type_ids = encode_example(question, 128)\n",
    "    predictions = loaded_model.predict({\n",
    "        'input_ids': tf.convert_to_tensor([input_ids], dtype=tf.int32),\n",
    "        'attention_mask': tf.convert_to_tensor([attention_mask], dtype=tf.int32),\n",
    "        'token_type_ids': tf.convert_to_tensor([token_type_ids], dtype=tf.int32)\n",
    "    })\n",
    "    prediction = labels[predictions[0].argmax().item()]\n",
    "    probability = predictions[0].max()\n",
    "    result = {\n",
    "        'prediction': str(labels[predictions[0].argmax().item()]),\n",
    "        'probability': str(predictions[0].max())\n",
    "    }\n",
    "    print('Prediction: {}'.format(prediction))\n",
    "    print('Probability: {}'.format(probability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiement with our new model\n",
    "\n",
    "Now we can easily test responses of the model to new inputs. \n",
    "*  **ACTION**: Invent yout own input for one of the 5 services our model understands: 'azure-web-app-service', 'azure-storage', 'azure-devops', 'azure-virtual-machine', 'azure-functions'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: azure-devops\n",
      "Probability: 0.2559393048286438\n"
     ]
    }
   ],
   "source": [
    "# Route question\n",
    "predict(\"How can I specify Service Principal in devops pipeline when deploying virtual machine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: azure-devops\n",
      "Probability: 0.2656690180301666\n"
     ]
    }
   ],
   "source": [
    "# Now more tricky cae - the opposite\n",
    "predict(\"How can virtual machine trigger devops pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Training Across Multiple GPUs\n",
    "\n",
    "Distributed training allows us to train across multiple nodes if your cluster allows it. Azure Machine Learning service helps manage the infrastructure for training distributed jobs. All we have to do is add the following parameters to our estimator object in order to enable this:\n",
    "\n",
    "- `node_count`: The number of nodes to run this job across. Our cluster has a maximum node limit of 2, so we can set this number up to 2.\n",
    "- `process_count_per_node`: The number of processes to enable per node. The nodes in our cluster have 2 GPUs each. We will set this value to 2 which will allow us to distribute the load on both GPUs. Using multi-GPUs nodes is benefitial as communication channel bandwidth on local machine is higher.\n",
    "- `distributed_training`: The backend to use for our distributed job. We will be using an MPI (Message Passing Interface) backend which is used by Horovod framework.\n",
    "\n",
    "We use [Horovod](https://github.com/horovod/horovod), which is a framework that allows us to easily modifying our existing training script to be run across multiple nodes/GPUs. The distributed training script is saved as *train_horovod.py*.\n",
    "\n",
    "*  **ACTION**: Explore _train_horovod.py_ using [Azure ML studio > Notebooks tab](images/azuremlstudio-notebooks-explore.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can submit this run in the same way that we did with the others, but with the additional parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import Mpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mpi_estimator = TensorFlow(source_directory='./',\n",
    "                        entry_script='train_horovod.py',compute_target=compute_target,\n",
    "                        script_params = {\n",
    "                              '--data_dir': azure_dataset.as_named_input('azureservicedata').as_mount(),\n",
    "                              '--max_seq_length': 128,\n",
    "                              '--batch_size': 32,\n",
    "                              '--learning_rate': 3e-5,\n",
    "                              '--steps_per_epoch': 150,\n",
    "                              '--num_epochs': 3,\n",
    "                              '--export_dir':'./outputs/model'\n",
    "                        },\n",
    "                        framework_version='2.0',\n",
    "                        node_count=1,\n",
    "                        distributed_training=Mpi(process_count_per_node=2),\n",
    "                        use_gpu=True,\n",
    "                        pip_packages=['transformers==2.0.0', 'azureml-dataprep[fuse,pandas]==1.1.38'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mpi_run = experiment.submit(mpi_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can view the current details of the run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "195b88d895b54fa3886e1ef8fc265b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": [
       "{\"status\": \"Queued\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/bert-classifier-test/runs/bert-classifier-test_1581362274_c3c0335b?wsid=/subscriptions/a89228ac-8bf4-4646-9d2c-442b1cb5d622/resourcegroups/lauri-ml/workspaces/lauri-ml\", \"run_id\": \"bert-classifier-test_1581362274_c3c0335b\", \"run_properties\": {\"run_id\": \"bert-classifier-test_1581362274_c3c0335b\", \"created_utc\": \"2020-02-10T19:17:59.461596Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"amlcompute\", \"ContentSnapshotId\": \"79a683a6-d41f-4549-9e79-2b532dee2221\", \"AzureML.DerivedImageName\": \"azureml/azureml_7fb53c78ac22ee1fbf3142e33301823e\", \"ProcessInfoFile\": \"azureml-logs/process_info.json\", \"ProcessStatusFile\": \"azureml-logs/process_status.json\"}, \"tags\": {\"_aml_system_ComputeTargetStatus\": \"{\\\"AllocationState\\\":\\\"steady\\\",\\\"PreparingNodeCount\\\":0,\\\"RunningNodeCount\\\":0,\\\"CurrentNodeCount\\\":2}\"}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": null, \"status\": \"Queued\", \"log_files\": {}, \"log_groups\": [], \"run_duration\": \"0:05:20\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"Your job is submitted in Azure cloud and we are monitoring to get logs...\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.83\"}, \"loading\": false}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RunDetails(mpi_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the run completes note the time it took. It should be around 5 minutes. As you can see, by moving to the cloud GPUs and using distibuted training we managed to reduce training time of our model from more than an hour to 5 minutes. This greatly improves speed of experimentation and innovation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters Using Hyperdrive\n",
    "\n",
    "So far we have been putting in default hyperparameter values, but in practice we would need tune these values to optimize the performance. Azure Machine Learning service provides many methods for tuning hyperparameters using different strategies.\n",
    "\n",
    "The first step is to choose the parameter space that we want to search. We have a few choices to make here :\n",
    "\n",
    "- **Parameter Sampling Method**: This is how we select the combinations of parameters to sample. Azure Machine Learning service offers [RandomParameterSampling](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.randomparametersampling?view=azure-ml-py), [GridParameterSampling](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.gridparametersampling?view=azure-ml-py), and [BayesianParameterSampling](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.bayesianparametersampling?view=azure-ml-py). We will use the `GridParameterSampling` method.\n",
    "- **Parameters To Search**: We will be searching for optimal combinations of `learning_rate` and `num_epochs`.\n",
    "- **Parameter Expressions**: This defines the [functions that can be used to describe a hyperparameter search space](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.parameter_expressions?view=azure-ml-py), which can be discrete or continuous. We will be using a `discrete set of choices`.\n",
    "\n",
    "The following code allows us to define these options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import GridParameterSampling\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_sampling = GridParameterSampling( {\n",
    "        '--learning_rate': choice(3e-5, 3e-4),\n",
    "        '--num_epochs': choice(3, 4)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to a define how we want to measure our performance. We do so by specifying two classes:\n",
    "\n",
    "- **[PrimaryMetricGoal](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.primarymetricgoal?view=azure-ml-py)**: We want to `MAXIMIZE` the `val_accuracy` that is logged in our training script.\n",
    "- **[BanditPolicy](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.banditpolicy?view=azure-ml-py)**: A policy for early termination so that jobs which don't show promising results will stop automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import BanditPolicy\n",
    "from azureml.train.hyperdrive import PrimaryMetricGoal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "primary_metric_name='val_accuracy'\n",
    "primary_metric_goal=PrimaryMetricGoal.MAXIMIZE\n",
    "\n",
    "early_termination_policy = BanditPolicy(slack_factor = 0.1, evaluation_interval=1, delay_evaluation=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an estimator as usual, but this time without the script parameters that we are planning to search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator4 = TensorFlow(source_directory='./',\n",
    "                        entry_script='train_logging.py',\n",
    "                        compute_target=compute_target,\n",
    "                        script_params = {\n",
    "                              '--data_dir': azure_dataset.as_named_input('azureservicedata').as_mount(),\n",
    "                              '--max_seq_length': 128,\n",
    "                              '--batch_size': 32,\n",
    "                              '--steps_per_epoch': 150,\n",
    "                              '--export_dir':'./outputs/model',\n",
    "                        },\n",
    "                        framework_version='2.0',\n",
    "                        use_gpu=True,\n",
    "                        pip_packages=['transformers==2.0.0', 'azureml-dataprep[fuse,pandas]==1.1.38'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add all our parameters in a [HyperDriveConfig](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py) class and submit it as a run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import HyperDriveConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyperdrive_run_config = HyperDriveConfig(\n",
    "    estimator=estimator4,\n",
    "    hyperparameter_sampling=param_sampling, \n",
    "    policy=early_termination_policy,\n",
    "    primary_metric_name=primary_metric_name, \n",
    "    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "    max_total_runs=10,\n",
    "    max_concurrent_runs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run4 = experiment.submit(hyperdrive_run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we view the details of our run this time, we will see information and metrics for every run in our hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fcb87cf7f54bc4b9c4e9827e8f8850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": [
       "{\"status\": \"Running\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/bert-classifier-test/runs/bert-classifier-test_1581363778485721?wsid=/subscriptions/a89228ac-8bf4-4646-9d2c-442b1cb5d622/resourcegroups/lauri-ml/workspaces/lauri-ml\", \"run_id\": \"bert-classifier-test_1581363778485721\", \"run_properties\": {\"run_id\": \"bert-classifier-test_1581363778485721\", \"created_utc\": \"2020-02-10T19:42:58.565668Z\", \"properties\": {\"primary_metric_config\": \"{\\\"name\\\": \\\"val_accuracy\\\", \\\"goal\\\": \\\"maximize\\\"}\", \"resume_from\": \"null\", \"runTemplate\": \"HyperDrive\", \"azureml.runsource\": \"hyperdrive\", \"platform\": \"AML\", \"ContentSnapshotId\": \"ddd80fe1-6f4b-417c-a185-8d51f470af65\"}, \"tags\": {\"max_concurrent_jobs\": \"2\", \"max_total_jobs\": \"10\", \"max_duration_minutes\": \"10080\", \"policy_config\": \"{\\\"name\\\": \\\"BANDIT\\\", \\\"properties\\\": {\\\"evaluation_interval\\\": 1, \\\"delay_evaluation\\\": 2, \\\"slack_factor\\\": 0.1}}\", \"generator_config\": \"{\\\"name\\\": \\\"GRID\\\", \\\"parameter_space\\\": {\\\"--learning_rate\\\": [\\\"choice\\\", [[3e-05, 0.0003]]], \\\"--num_epochs\\\": [\\\"choice\\\", [[3, 4]]]}}\", \"primary_metric_config\": \"{\\\"name\\\": \\\"val_accuracy\\\", \\\"goal\\\": \\\"maximize\\\"}\", \"platform_config\": \"{\\\"ServiceAddress\\\": \\\"https://westeurope.experiments.azureml.net\\\", \\\"ServiceArmScope\\\": \\\"subscriptions/a89228ac-8bf4-4646-9d2c-442b1cb5d622/resourceGroups/lauri-ml/providers/Microsoft.MachineLearningServices/workspaces/lauri-ml/experiments/bert-classifier-test\\\", \\\"SubscriptionId\\\": \\\"a89228ac-8bf4-4646-9d2c-442b1cb5d622\\\", \\\"ResourceGroupName\\\": \\\"lauri-ml\\\", \\\"WorkspaceName\\\": \\\"lauri-ml\\\", \\\"ExperimentName\\\": \\\"bert-classifier-test\\\", \\\"Definition\\\": {\\\"Overrides\\\": {\\\"script\\\": \\\"train_logging.py\\\", \\\"arguments\\\": [\\\"--data_dir\\\", \\\"DatasetConsumptionConfig:azureservicedata\\\", \\\"--max_seq_length\\\", 128, \\\"--batch_size\\\", 32, \\\"--steps_per_epoch\\\", 150, \\\"--export_dir\\\", \\\"./outputs/model\\\"], \\\"target\\\": \\\"bert-gpu\\\", \\\"framework\\\": \\\"Python\\\", \\\"communicator\\\": \\\"None\\\", \\\"maxRunDurationSeconds\\\": null, \\\"nodeCount\\\": 1, \\\"environment\\\": {\\\"name\\\": null, \\\"version\\\": null, \\\"environmentVariables\\\": {\\\"EXAMPLE_ENV_VAR\\\": \\\"EXAMPLE_VALUE\\\"}, \\\"python\\\": {\\\"userManagedDependencies\\\": false, \\\"interpreterPath\\\": \\\"python\\\", \\\"condaDependenciesFile\\\": null, \\\"baseCondaEnvironment\\\": null, \\\"condaDependencies\\\": {\\\"name\\\": \\\"project_environment\\\", \\\"dependencies\\\": [\\\"python=3.6.2\\\", {\\\"pip\\\": [\\\"transformers==2.0.0\\\", \\\"azureml-dataprep[fuse,pandas]==1.1.38\\\", \\\"azureml-defaults\\\", \\\"tensorflow-gpu==2.0.0\\\", \\\"horovod==0.18.1\\\"]}], \\\"channels\\\": [\\\"conda-forge\\\"]}}, \\\"docker\\\": {\\\"enabled\\\": true, \\\"baseImage\\\": \\\"mcr.microsoft.com/azureml/base-gpu:openmpi3.1.2-cuda10.0-cudnn7-ubuntu18.04\\\", \\\"baseDockerfile\\\": null, \\\"sharedVolumes\\\": true, \\\"shmSize\\\": \\\"2g\\\", \\\"arguments\\\": [], \\\"baseImageRegistry\\\": {\\\"address\\\": null, \\\"username\\\": null, \\\"password\\\": null}}, \\\"spark\\\": {\\\"repositories\\\": [], \\\"packages\\\": [], \\\"precachePackages\\\": false}, \\\"databricks\\\": {\\\"mavenLibraries\\\": [], \\\"pypiLibraries\\\": [], \\\"rcranLibraries\\\": [], \\\"jarLibraries\\\": [], \\\"eggLibraries\\\": []}, \\\"inferencingStackVersion\\\": null}, \\\"history\\\": {\\\"outputCollection\\\": true, \\\"snapshotProject\\\": true, \\\"directoriesToWatch\\\": [\\\"logs\\\"]}, \\\"spark\\\": {\\\"configuration\\\": {\\\"spark.app.name\\\": \\\"Azure ML Experiment\\\", \\\"spark.yarn.maxAppAttempts\\\": 1}}, \\\"hdi\\\": {\\\"yarnDeployMode\\\": \\\"cluster\\\"}, \\\"tensorflow\\\": {\\\"workerCount\\\": 1, \\\"parameterServerCount\\\": 1}, \\\"mpi\\\": {\\\"processCountPerNode\\\": 1}, \\\"dataReferences\\\": {}, \\\"data\\\": {\\\"azureservicedata\\\": {\\\"dataLocation\\\": {\\\"dataset\\\": {\\\"id\\\": \\\"9e790eda-004e-4777-9865-02446ae26898\\\"}, \\\"datapath\\\": null}, \\\"createOutputDirectories\\\": false, \\\"mechanism\\\": \\\"mount\\\", \\\"environmentVariableName\\\": \\\"azureservicedata\\\", \\\"pathOnCompute\\\": null, \\\"overwrite\\\": false}}, \\\"sourceDirectoryDataStore\\\": null, \\\"amlcompute\\\": {\\\"vmSize\\\": null, \\\"vmPriority\\\": null, \\\"retainCluster\\\": false, \\\"name\\\": null, \\\"clusterMaxNodeCount\\\": 1}}, \\\"TargetDetails\\\": null, \\\"SnapshotId\\\": \\\"ddd80fe1-6f4b-417c-a185-8d51f470af65\\\", \\\"TelemetryValues\\\": {\\\"amlClientType\\\": \\\"azureml-sdk-train\\\", \\\"amlClientModule\\\": \\\"azureml.train.hyperdrive._search\\\", \\\"amlClientFunction\\\": \\\"search\\\", \\\"tenantId\\\": \\\"d4842539-1664-4a55-981e-a6a6643b6d02\\\", \\\"amlClientRequestId\\\": \\\"055fcb69-27e0-4c54-bb60-23071cce483b\\\", \\\"amlClientSessionId\\\": \\\"75ea7e84-9ad5-43d0-83de-7017b33ee0aa\\\", \\\"subscriptionId\\\": \\\"a89228ac-8bf4-4646-9d2c-442b1cb5d622\\\", \\\"estimator\\\": \\\"TensorFlow\\\", \\\"samplingMethod\\\": \\\"GRID\\\", \\\"terminationPolicy\\\": \\\"Bandit\\\", \\\"primaryMetricGoal\\\": \\\"maximize\\\", \\\"maxTotalRuns\\\": 10, \\\"maxConcurrentRuns\\\": 2, \\\"maxDurationMinutes\\\": 10080, \\\"computeTarget\\\": \\\"AmlCompute\\\", \\\"vmSize\\\": null}}}\", \"resume_child_runs\": \"null\", \"all_jobs_generated\": \"false\", \"cancellation_requested\": \"false\", \"progress_metadata_evaluation_timestamp\": \"\\\"2020-02-10T19:42:59.123650\\\"\", \"progress_metadata_digest\": \"\\\"847133cf7b4decff1437436ae7d788f1d2a05cf61b224da863de8c08bd0e4de6\\\"\", \"progress_metadata_active_timestamp\": \"\\\"2020-02-10T19:42:59.123650\\\"\", \"bert-classifier-test_1581363778485721_0\": \"{\\\"--learning_rate\\\": 3e-05, \\\"--num_epochs\\\": 3}\", \"bert-classifier-test_1581363778485721_1\": \"{\\\"--learning_rate\\\": 0.0003, \\\"--num_epochs\\\": 3}\", \"environment_preparation_status\": \"PREPARED\", \"prepare_run_id\": \"bert-classifier-test_1581363778485721_preparation\"}, \"end_time_utc\": null, \"status\": \"Running\", \"log_files\": {\"azureml-logs/hyperdrive.txt\": \"https://lauriml7558265257.blob.core.windows.net/azureml/ExperimentRun/dcid.bert-classifier-test_1581363778485721/azureml-logs/hyperdrive.txt?sv=2019-02-02&sr=b&sig=lI8PLYeQvQPZJCk9K%2FsjScHam8Z2gLAlPWn1kbX1BxE%3D&st=2020-02-10T19%3A36%3A20Z&se=2020-02-11T03%3A46%3A20Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/hyperdrive.txt\"]], \"run_duration\": \"0:03:21\", \"hyper_parameters\": {\"--learning_rate\": [\"choice\", [[3e-05, 0.0003]]], \"--num_epochs\": [\"choice\", [[3, 4]]]}}, \"child_runs\": [{\"run_id\": \"bert-classifier-test_1581363778485721_1\", \"run_number\": 22, \"metric\": null, \"status\": \"Running\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2020-02-10T19:44:02.815952Z\", \"end_time\": \"\", \"created_time\": \"2020-02-10T19:43:31.162788Z\", \"created_time_dt\": \"2020-02-10T19:43:31.162788Z\", \"duration\": \"0:02:49\", \"hyperdrive_id\": \"1581363778485721\", \"arguments\": null, \"param_--learning_rate\": 0.0003, \"param_--num_epochs\": 3}, {\"run_id\": \"bert-classifier-test_1581363778485721_0\", \"run_number\": 23, \"metric\": null, \"status\": \"Running\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2020-02-10T19:44:02.471024Z\", \"end_time\": \"\", \"created_time\": \"2020-02-10T19:43:31.176955Z\", \"created_time_dt\": \"2020-02-10T19:43:31.176955Z\", \"duration\": \"0:02:49\", \"hyperdrive_id\": \"1581363778485721\", \"arguments\": null, \"param_--learning_rate\": 3e-05, \"param_--num_epochs\": 3}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2020-02-10T19:42:58.754587][API][INFO]Experiment created\\r\\n[2020-02-10T19:42:59.817313][GENERATOR][INFO]Trying to sample '2' jobs from the hyperparameter space\\r\\n[2020-02-10T19:43:00.0731621Z][SCHEDULER][INFO]The execution environment is being prepared. Please be patient as it can take a few minutes.\\r\\n[2020-02-10T19:43:00.101551][GENERATOR][INFO]Successfully sampled '2' jobs, they will soon be submitted to the execution target.\\r\\n[2020-02-10T19:43:30.5424502Z][SCHEDULER][INFO]Scheduling job, id='bert-classifier-test_1581363778485721_0'\\r\\n[2020-02-10T19:43:30.5435209Z][SCHEDULER][INFO]Scheduling job, id='bert-classifier-test_1581363778485721_1'\\r\\n[2020-02-10T19:43:30.5420323Z][SCHEDULER][INFO]The execution environment was successfully prepared.\\r\\n[2020-02-10T19:43:31.2858192Z][SCHEDULER][INFO]Successfully scheduled a job. Id='bert-classifier-test_1581363778485721_0'\\r\\n[2020-02-10T19:43:31.2343174Z][SCHEDULER][INFO]Successfully scheduled a job. Id='bert-classifier-test_1581363778485721_1'\\r\\n[2020-02-10T19:44:28.440632][ENFORCER][INFO]Jobs [RunKey(run_scope=RunScope(host='https://westeurope.experiments.azureml.net', subscription_id='a89228ac-8bf4-4646-9d2c-442b1cb5d622', resource_group='lauri-ml', workspace_name='lauri-ml', project_name='bert-classifier-test'), run_id='bert-classifier-test_1581363778485721_0', data_container_id=None, baggage=None), RunKey(run_scope=RunScope(host='https://westeurope.experiments.azureml.net', subscription_id='a89228ac-8bf4-4646-9d2c-442b1cb5d622', resource_group='lauri-ml', workspace_name='lauri-ml', project_name='bert-classifier-test'), run_id='bert-classifier-test_1581363778485721_1', data_container_id=None, baggage=None)] do not contain any metrics with the primary metric name at this moment, policy cannot be applied.\\r\\n[2020-02-10T19:44:58.421294][ENFORCER][INFO]Jobs [RunKey(run_scope=RunScope(host='https://westeurope.experiments.azureml.net', subscription_id='a89228ac-8bf4-4646-9d2c-442b1cb5d622', resource_group='lauri-ml', workspace_name='lauri-ml', project_name='bert-classifier-test'), run_id='bert-classifier-test_1581363778485721_0', data_container_id=None, baggage=None), RunKey(run_scope=RunScope(host='https://westeurope.experiments.azureml.net', subscription_id='a89228ac-8bf4-4646-9d2c-442b1cb5d622', resource_group='lauri-ml', workspace_name='lauri-ml', project_name='bert-classifier-test'), run_id='bert-classifier-test_1581363778485721_1', data_container_id=None, baggage=None)] do not contain any metrics with the primary metric name at this moment, policy cannot be applied.\\r\\n[2020-02-10T19:45:28.562209][ENFORCER][INFO]Jobs [RunKey(run_scope=RunScope(host='https://westeurope.experiments.azureml.net', subscription_id='a89228ac-8bf4-4646-9d2c-442b1cb5d622', resource_group='lauri-ml', workspace_name='lauri-ml', project_name='bert-classifier-test'), run_id='bert-classifier-test_1581363778485721_0', data_container_id=None, baggage=None), RunKey(run_scope=RunScope(host='https://westeurope.experiments.azureml.net', subscription_id='a89228ac-8bf4-4646-9d2c-442b1cb5d622', resource_group='lauri-ml', workspace_name='lauri-ml', project_name='bert-classifier-test'), run_id='bert-classifier-test_1581363778485721_1', data_container_id=None, baggage=None)] do not contain any metrics with the primary metric name at this moment, policy cannot be applied.\\r\\n[2020-02-10T19:45:58.992601][ENFORCER][INFO]Jobs [RunKey(run_scope=RunScope(host='https://westeurope.experiments.azureml.net', subscription_id='a89228ac-8bf4-4646-9d2c-442b1cb5d622', resource_group='lauri-ml', workspace_name='lauri-ml', project_name='bert-classifier-test'), run_id='bert-classifier-test_1581363778485721_0', data_container_id=None, baggage=None), RunKey(run_scope=RunScope(host='https://westeurope.experiments.azureml.net', subscription_id='a89228ac-8bf4-4646-9d2c-442b1cb5d622', resource_group='lauri-ml', workspace_name='lauri-ml', project_name='bert-classifier-test'), run_id='bert-classifier-test_1581363778485721_1', data_container_id=None, baggage=None)] do not contain any metrics with the primary metric name at this moment, policy cannot be applied.\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.83\"}, \"loading\": false}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(run4).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the best run based on our defined metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_run = run4.get_best_run_by_primary_metric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Model\n",
    "\n",
    "A registered [model](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model(class)?view=azure-ml-py) is a reference to the directory or file that make up your model. After registering a model, you and other people in your workspace can easily gain access to and deploy your model without having to run the training script again. \n",
    "\n",
    "We need to define the following parameters to register a model:\n",
    "\n",
    "- `model_name`: The name for your model. If the model name already exists in the workspace, it will create a new version for the model.\n",
    "- `model_path`: The path to where the model is stored. In our case, this was the *export_dir* defined in our estimators.\n",
    "- `description`: A description for the model.\n",
    "\n",
    "Let's register the best run from our hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'azure-service-classifier'\n",
    "model_name = experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = best_run.register_model(model_name=model_name, \n",
    "                                model_path='./outputs/model',\n",
    "                                datasets=[('train, test, validation data', azure_dataset)],\n",
    "                                description='BERT model for classifying azure services on stackoverflow posts.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have registered the model with Dataset reference. \n",
    "* **ACTION**: Check dataset to model link in **Azure ML studio > Datasets tab > Azure Service Dataset**."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bitc58bf88b8f1e45baa78bacf854e56ebc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}